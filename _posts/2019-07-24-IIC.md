---
title: Invariant Information Clustering for Unsupervised Image Classification and Segmentation
date: 2019.07.24 12:00:00
categories:
- Paper Review
tags:
- Image classification
---

## Abstract

- 레이블링된 데이터가 없고 데이터 그 자체만 있는 경우 클러스터링을 진행할 수 있는 방법론에 대해 다룸.
- 이 모델을 통해서 8개 정도의 테스크에서 SOTA를 찍었고, semantic 측면에서도 적합한 클래스를 분류해 내었다고 한다.
- 단순히 비전 테스크에서만 동작하는 것이 아니라 짝이 지어진 데이터셋에서 모두 동작할 수 있는 컨셉이라고 함.
- 데이터셋을 구성하기 위해서 저자는 random transformation을 사용해서 쌍을 만들어 냄.
- 학습된 모델은 결과물로 추가적인 클러스터링의 소스로 사용될 수 있는 고차원의 representation이 아닌
semantic label을 뱉어내게 됨.
- 목적함수는 각 쌍의 클래스 배정에 있어 상호 정보력을 최대화 하는 것임.
- 정보 이론에 그 기반을 두고 있어 쉽게 구현할 수 있고, 다른 클러스터링 방법론들이 쉽사리 결과물로 내뱉는
degenerate solutions을 피할 수 있다고 한다.
- 완벽한 Unsupervised 컨셉에 더해 저자는 2가지 옵션의 semi 옵션을 추가해서 학습을 진행했고, 이 또한
좋은 성능을 냈다고 함.

## 1. Introduction

- 대부분의 Supervised 컨셉의 방법론들은 많은 양의 태깅된 데이터셋이 필요함 (Image classification, segmentation)
- 태깅(labeling)을 진행하는 비용은 비싸기 때문에 이런 Unsupervised 컨셉의 방법론들에 대한 시도가 계속 이루어짐.
- 기존까지의 접근들은 clustering + representation learning인데, 이는 저자의 표현에 따르면 `degenerate solution`이라는
결과를 낳는다.
- 정확히는 degeneracy를 방지하기 위해 귀찮은 파이프라인들을 추가하는 과정들을 거친다 (pretraining, feature post-processing 등)
- 저자가 제안한 IIC는 보다 근본적인 부분에 집중을 한다.
- IIC는 랜덤 초기화된 뉴럴넷을 classification function이 되도록 학습시킨다.
- 이 과정에는 간단한 목적함수가 포함되어 있는데, 이는 앞서 말한 function이 각 쌍의 데이터에 대해서 예측한 분류 결과들 사이의 상호 정보를 포함한다.
- 그렇기 때문에 인풋이 어떤 데이터가 되든 간에 학습이 가능한 컨셉이라고 말한다.
- 굉장히 간단해 보여서 잘 동작할까 의심스러워할까 걱정되었는지, 저자는 본질적으로 이 방법론이 견고한 2가지 이유를 든다.

#### 1. degenerated solution

- 여기서 앞에서 말했던 degenerate solution이 뭔지 조금 더 자세히 풀어서 말해주는데 쉽게 말하면 클러스터링의 결과가 한 클러스터로 쏠리는 경우를 말한다고 한다.
- 이런 현상은 k-means에서 주로 관찰되며, representation learning과 합쳐졌을 때 더 심해진다고 한다.
  - 관련해서 Deepcluster를 지속적으로 저격한다.
- Mutual Information 안에 있는 엔트로피 최대화 때문에 앞서 말한 경우처럼 한 클러스터로 쏠림 현상이 일어났을 경우에는
loss function이 줄어드지 않는 결과를 낳게 된다.
- 그와 동시에 모델의 입장에서는 이미지를 특정 클래스에 확실히 배정하는게 conditional entropy minimisation 관점에서 optimal 하다.

#### 2. Noisy data

- 데이터 셋에 알수 없는 데이터가 섞여 있거나, 방해하는 역할을 하는 클래스가 섞여 있을 경우를 뜻함.
- IIC는 보조 아웃풋 레이어를 적용함으로써 (+ main output layer와 병렬로) overclustering을 하도록 학습시킨다고 한다.
- 같은 손실함수를 쓰지만 실제 ground truth 클래스보다 더 많은 수의 클러스터로 분류하도록 하고, 테스트 환경에서는 제외.
- 이 방식들을 통해서 ImageNet의 Unsupervised 버전인 STL10에서 견고한 성능을 보였다고 함.


## 2. Related works

생략

## 3. Method

### 3.1. Invariant Information Clustering

세부 내용은 수식으로 대신한다.

#### MI를 최대화 시키는 과정을 통해서 어떻게 degenerated solution을 피할 수 있는가?

- 위의 수식을 보면 결국 MI는 앞의 엔트로피와 conditional entropy로 구성된다.
- MI의 최대화는 엔트로피의 최대화를 뜻하고, 결국 H(z)는 lnC라는 최대값을 가지기 위해 움직인다.
- $$H(z \mid z')$$은 최소화되어야 하며, 결국 0에 수렴하는 방향으로 이동한다.
  - cluster assignments가 정확히 한 쌍 간에 예측이 가능하다는 뜻.
- degenerated solution의 주된 예시인 한 클러스터에 모든 입력 값이 쏠리는 경우는 H(z) 최대화 측면에서 로스가 줄어들지 않는 방식으로
방지가 가능하다.

#### MI의 의미

- 그렇다면 위에서 말한 degraded solution을 막기 위해서라면 단순히 엔트로피만 사용하면 되지 않는가?
- 만약 엔트로피만 쓰게 되면 클러스터링을 진행하는 것이 아니라 단순히 uniform distribution만 구성해도 엔트로피가 최대화 되기 때문에,
클러스터링이 정상적으로 이루어지지 않게된다.
- 이 부분을 위 수식의 두번째 부분인 conditional entropy가 잡아준다.
- 결과론적인 것이긴 하지만 IIC는 결국 `x=x'`인 케이스에 특정 클러스터로 할당되게 할 것이고 결론적으로 $$H(z \mid z')=0$$이라는 결과로 귀결된다.

### 3.2. Image clustering

- IIC는 쌍으로 만들어진 데이터셋이 필요하다.
- 이를 위해서 randomly pertubed version을 만들어 `x' = gx`를 만들어 낸다.
  - scaling, skewing, rotation, flipping, changing contrast/colour.

#### Auxiliary overclustering

- STL10과 같은 데이터에는 2가지 종류가 있다고 함
  - 특정 클래스라고 알려진 데이터
  - 관계없는 클래스라고 알려져있거나, distractor 클래스로 알려진 데이터
- 특정 클래스라고 알려진 데이터만 가지고 학습을 진행하고 싶지만, 후자에 속하는 데이터의 크기가 너무 커서 버리기 아까움.
  - known: 13K, unknonw/distractor: 100K
- 그래서 auxiliary overclustering을 사용함 -> 두개의 데이터를 다 사용해보자!
- 전체 데이터셋을 이용해서 overclustering head를 학습.
- main output head는 특정 클래스라고 알려진 데이터에 대해서 학습.
- 이를 통해 noisy 데이터를 학습에 사용할 수 있다는 장점.
- 그동안 다른 방법들은 이 아까운 데이터들을 다 버리고 학습.
- 결론적으로 학습하는 representation의 표현력을 높일 수 있다고 주장. (꼭 distractor class가 없다고 하더라도)
